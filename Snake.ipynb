{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "#os.putenv('SDL_VIDEODRIVER', 'fbcon')\n",
    "import random\n",
    "import pygame\n",
    "import math\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.color import rgb2gray\n",
    "import torchvision.transforms as T\n",
    "from collections import namedtuple,deque\n",
    "\n",
    "import torch.nn as nn\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import gc\n",
    "#gc.collect()\n",
    "\n",
    "\n",
    "class Cube(object):\n",
    "    rows=20\n",
    "    w=200\n",
    "\n",
    "    def __init__(self,start,dirnx=0,dirny=1,color=(255,0,0)):\n",
    "        #dirnx 0: dont move in x direction; 1: move right; -1: move left\n",
    "        #dirny 0: dont move in y direction; 1: move down; -1 move up\n",
    "\n",
    "        self.pos=start\n",
    "        self.dirnx=dirnx\n",
    "        self.dirny=dirny\n",
    "        self.color=color\n",
    "\n",
    "\n",
    "    def move(self,dirnx,dirny):\n",
    "        self.dirnx=dirnx\n",
    "        self.dirny=dirny\n",
    "        self.pos=(self.pos[0]+self.dirnx,self.pos[1]+self.dirny)\n",
    "\n",
    "    def draw(self,surface,eyes=False):\n",
    "        dis=self.w//self.rows\n",
    "        i=self.pos[0]\n",
    "        j=self.pos[1]\n",
    "        pygame.draw.rect(surface,self.color,(i*dis+1,j*dis+1,dis-2,dis-2))\n",
    "        if eyes:\n",
    "            center=dis//2\n",
    "            radius=1\n",
    "            circleMiddle=(i*dis+center-radius-1,j*dis+2)\n",
    "            circleMiddle2 = (i * dis +center+ radius+1, j * dis + 2)\n",
    "            pygame.draw.circle(surface,(0,0,0),circleMiddle,radius)\n",
    "            pygame.draw.circle(surface, (0, 0, 0), circleMiddle2, radius)\n",
    "\n",
    "# if __name__=='__main__':\n",
    "#     width = 500\n",
    "#     rows = 20\n",
    "#     status=[]\n",
    "#     actions=[]\n",
    "#     rewards=[]\n",
    "#     win = pygame.display.set_mode((500, 600))\n",
    "#     dirnx, dirny = random.sample(((-1, 0), (1, 0), (0, 1), (0, -1)), 1)[0]\n",
    "#     s = Snake((255, 0, 0), (random.randint(5, 15), random.randint(5, 15)), dirnx, dirny)\n",
    "#     snack = Cube(randomSnack(rows, s), color=(0, 255, 0))\n",
    "#     flag = True\n",
    "#     clock = pygame.time.Clock()\n",
    "#     k=5\n",
    "#     steps=k-1\n",
    "#     i=1\n",
    "#     while flag and i:\n",
    "#\n",
    "#         pygame.time.delay(50)\n",
    "#         clock.tick(10)\n",
    "#         redrawWindow(win, rows, width, s, snack)\n",
    "#\n",
    "#         status.append(pygame.surfarray.array3d(win))\n",
    "#         AgentInput=random.randint(0,3)\n",
    "#         actions.append(AgentInput)\n",
    "#\n",
    "#         s.move('Agent',AgentInput)\n",
    "#         print(steps)\n",
    "#         if AgentInput==0:\n",
    "#             print('up')\n",
    "#         elif AgentInput==1:\n",
    "#             print('down')\n",
    "#         elif AgentInput==2:\n",
    "#             print('left')\n",
    "#         else:\n",
    "#             print('right')\n",
    "#         if s.body[0].pos == snack.pos:\n",
    "#             rewards.append(1)\n",
    "#\n",
    "#             steps+=10\n",
    "#             s.addCube()\n",
    "#             snack = Cube(randomSnack(rows, s), color=(0, 255, 0))\n",
    "#         else:\n",
    "#\n",
    "#\n",
    "#             for x in range(len(s.body)):\n",
    "#                 if s.body[x].pos in list(map(lambda z: z.pos, s.body[x + 1:])) or s.head.pos[0] < 0 or s.head.pos[0] > 19 or \\\n",
    "#                         s.head.pos[1] < 0 or s.head.pos[1] > 19 or not steps:\n",
    "#                     rewards.append(-1)\n",
    "#                     print('Score: ', len(s.body)-1)\n",
    "#                     #message_box('You Lost', 'Play again')\n",
    "#                     dirnx, dirny = random.sample(((-1, 0), (1, 0), (0, 1), (0, -1)), 1)[0]\n",
    "#                     s.reset((random.randint(5, 15), random.randint(5, 15)), dirnx, dirny)\n",
    "#                     status.append(None)\n",
    "#                     steps=k+1\n",
    "#                     i-=1\n",
    "#                     break\n",
    "#                 else:\n",
    "#                     rewards.append(0)\n",
    "#\n",
    "#         steps -= 1\n",
    "#         redrawWindow(win, rows, width, s, snack)\n",
    "#     pygame.quit()\n",
    "#\n",
    "#     # k=1\n",
    "#     # x=type(states[0])\n",
    "#     # for img in states:\n",
    "#     #     if type(img)==x:\n",
    "#     #         plt.figure(k)\n",
    "#     #         k+=1\n",
    "#     #         plt.imshow(img.transpose(1,0,2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# curr_state=[]\n",
    "# action=[]\n",
    "# reward=[]\n",
    "# next_state=[]\n",
    "# s=env.get_curr_frame()\n",
    "# Transition=namedtuple('Transition',\n",
    "#                         ('state', 'action', 'reward', 'next_state'))\n",
    "# t=[]\n",
    "# tp=type(s)\n",
    "# for j in range(2):\n",
    "#     s = env.get_curr_frame()\n",
    "#     #print('j:' ,j)\n",
    "#     for i in range(10):\n",
    "#         #print('i: ',i)\n",
    "#         env.render()\n",
    "#         a=random.randint(0,3)\n",
    "#         r,ns=env.step(a,'Agent')\n",
    "#         memory.push(s,a,r,ns)\n",
    "#         print(env.steps)\n",
    "#         if ns is None:\n",
    "#             print('reset')\n",
    "#             env.reset()\n",
    "#             break\n",
    "#         s=ns\n",
    "#pygame.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.imshow(curr_state_pytorch.squeeze(0).permute(1, 2, 0).numpy(),\n",
    "#            interpolation='none')\n",
    "# plt.title('Example extracted screen')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Snake(object):\n",
    "    body=[]\n",
    "    turns={}\n",
    "    def __init__(self,color,pos,dirnx,dirny):\n",
    "        self.color=color\n",
    "        self.dirnx, self.dirny = dirnx,dirny\n",
    "        self.head = Cube(pos,dirnx,dirny)\n",
    "        self.body.append(self.head)\n",
    "    def move(self,mode='Player',AgentInput=0):\n",
    "\n",
    "        #the mode can be Player or Agent, Agent is DQN\n",
    "        #when the mode is Player, use keyboard to play\n",
    "        #when the mode is Agent, the AgentInput will be 0,1,2,3 ->up, down, left, right\n",
    "\n",
    "        if mode=='Player':\n",
    "            for event in pygame.event.get():\n",
    "                if event.type==pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    sys.exit()\n",
    "                keys=pygame.key.get_pressed()\n",
    "                #for key in keys:\n",
    "                if keys[pygame.K_LEFT] and (len(self.body)==1 or (len(self.body)>1 and not (self.dirnx==1 and self.dirny==0))):\n",
    "                    self.dirnx=-1\n",
    "                    self.dirny=0\n",
    "                    self.turns[self.head.pos[:]]=[self.dirnx,self.dirny]\n",
    "\n",
    "\n",
    "                elif keys[pygame.K_RIGHT] and (len(self.body)==1 or (len(self.body)>1 and not (self.dirnx==-1 and self.dirny==0))):\n",
    "\n",
    "                    self.dirnx=1\n",
    "                    self.dirny=0\n",
    "                    self.turns[self.head.pos[:]]=[self.dirnx,self.dirny]\n",
    "\n",
    "\n",
    "                elif keys[pygame.K_UP] and (len(self.body) == 1 or (len(self.body)>1 and not (self.dirnx==0 and self.dirny==1))):\n",
    "                    self.dirnx=0\n",
    "                    self.dirny=-1\n",
    "                    self.turns[self.head.pos[:]]=[self.dirnx,self.dirny]\n",
    "\n",
    "                elif keys[pygame.K_DOWN] and (len(self.body) == 1 or (len(self.body)>1 and not (self.dirnx==0 and self.dirny==-1))):\n",
    "\n",
    "                    self.dirnx=0\n",
    "                    self.dirny=1\n",
    "                    self.turns[self.head.pos[:]]=[self.dirnx,self.dirny]\n",
    "\n",
    "\n",
    "        elif mode=='Agent':\n",
    "            if AgentInput==0 and (len(self.body) == 1 or (len(self.body)>1 and not (self.dirnx==0 and self.dirny==1))) :\n",
    "                self.dirnx=0\n",
    "                self.dirny=-1\n",
    "                self.turns[self.head.pos[:]]=[self.dirnx,self.dirny]\n",
    "                \n",
    "            elif AgentInput==1 and (len(self.body) == 1 or (len(self.body)>1 and not (self.dirnx==0 and self.dirny==-1))):\n",
    "                self.dirnx = 0\n",
    "                self.dirny = 1\n",
    "                self.turns[self.head.pos[:]] = [self.dirnx, self.dirny]\n",
    "                \n",
    "            elif AgentInput==2 and (len(self.body)==1 or (len(self.body)>1 and not (self.dirnx==1 and self.dirny==0))):\n",
    "                self.dirnx = -1\n",
    "                self.dirny = 0\n",
    "                self.turns[self.head.pos[:]] = [self.dirnx, self.dirny]\n",
    "                \n",
    "            elif AgentInput==3 and (len(self.body)==1 or (len(self.body)>1 and not (self.dirnx==-1 and self.dirny==0))):\n",
    "                self.dirnx = 1\n",
    "                self.dirny = 0\n",
    "                self.turns[self.head.pos[:]] = [self.dirnx, self.dirny]\n",
    "\n",
    "        else:\n",
    "            print('Invalid mode, Player or Agent')\n",
    "\n",
    "        for i,c in enumerate(self.body):\n",
    "            #print('length of turns',len(self.turns))\n",
    "            p=c.pos[:]\n",
    "            if p in self.turns:\n",
    "                turn=self.turns[p]\n",
    "                c.move(turn[0],turn[1])\n",
    "                if i==len(self.body)-1:\n",
    "                    self.turns.pop(p)\n",
    "            else:\n",
    "                # if c.dirnx==-1 and c.pos[0]<=0: c.pos=(c.rows-1,c.pos[1])\n",
    "                # elif c.dirnx==1 and c.pos[0]>=c.rows-1: c.pos=(0,c.pos[1])\n",
    "                # elif c.dirny==1 and c.pos[1]>=c.rows-1:c.pos=(c.pos[0],0)\n",
    "                # elif c.dirny==-1 and c.pos[1]<=0:c.pos=(c.pos[0],c.rows-1)\n",
    "                # else: c.move(c.dirnx,c.dirny)\n",
    "                c.move(c.dirnx, c.dirny)\n",
    "\n",
    "\n",
    "    def reset(self,pos,dirnx,dirny):\n",
    "        self.head=Cube(pos,dirnx,dirny)\n",
    "        self.body=[]\n",
    "        self.body.append(self.head)\n",
    "        self.turns={}\n",
    "        self.dirnx=dirnx\n",
    "        self.dirny=dirny\n",
    "\n",
    "    def addCube(self):\n",
    "        tail=self.body[-1]\n",
    "        dx,dy=tail.dirnx,tail.dirny\n",
    "        if dx==1 and dy==0:self.body.append(Cube((tail.pos[0]-1,tail.pos[1])))\n",
    "        elif dx==-1 and dy==0:self.body.append(Cube((tail.pos[0]+1,tail.pos[1])))\n",
    "        elif dx == 0 and dy == 1:self.body.append(Cube((tail.pos[0], tail.pos[1]-1)))\n",
    "        elif dx == 0 and dy == -1:self.body.append(Cube((tail.pos[0], tail.pos[1] + 1)))\n",
    "\n",
    "        self.body[-1].dirnx=dx\n",
    "        self.body[-1].dirny=dy\n",
    "\n",
    "    def draw(self,surface):\n",
    "        for i,c in enumerate(self.body):\n",
    "            if i==0:\n",
    "                c.draw(surface,True)\n",
    "            else:\n",
    "                c.draw(surface)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    r=0.01\n",
    "\n",
    "    def __init__(self,screen_width=500,screen_height=500,rows=20,steps=20,reward_steps=200):\n",
    "        self.next_state=None\n",
    "        self.score=0\n",
    "        self.reward=0\n",
    "        self.steps=steps\n",
    "        self.stepcount=steps\n",
    "        self.reward_steps=reward_steps\n",
    "        self.screen_width=screen_width\n",
    "        self.screen_height=screen_height\n",
    "        self.win = pygame.display.set_mode((self.screen_width,self.screen_height))\n",
    "        self.dirnx, self.dirny = random.sample(((-1, 0), (1, 0), (0, 1), (0, -1)), 1)[0]\n",
    "        self.snake=Snake((255, 0, 0), (random.randint(5, 15), random.randint(5, 15)), self.dirnx, self.dirny)\n",
    "        self.rows=rows\n",
    "        self.snack = Cube(randomSnack(self.rows, self.snake), color=(0, 255, 0))\n",
    "        self.stack=[]\n",
    "\n",
    "    def reset(self):\n",
    "        self.dirnx, self.dirny = random.sample(((-1, 0), (1, 0), (0, 1), (0, -1)), 1)[0]\n",
    "        self.snake.reset((random.randint(5, 15), random.randint(5, 15)), self.dirnx, self.dirny)\n",
    "        self.snack = Cube(randomSnack(self.rows, self.snake), color=(0, 255, 0))\n",
    "        self.score=0\n",
    "        self.steps=self.stepcount\n",
    "        self.reward=0\n",
    "\n",
    "    def render(self,display=False,fps=10000):\n",
    "        clock = pygame.time.Clock()\n",
    "        #pygame.time.delay(10)\n",
    "        clock.tick(fps)\n",
    "\n",
    "        self.win.fill((0,0,0))\n",
    "        self.snake.draw(self.win)\n",
    "        self.snack.draw(self.win)\n",
    "\n",
    "\n",
    "        if display==True:\n",
    "            pygame.display.set_caption('Score: '+str(self.score))\n",
    "        pygame.display.update()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_curr_frame(self):\n",
    "        frame=pygame.surfarray.array3d(self.win)\n",
    "        #frame=rgb2gray(frame)\n",
    "        frame=frame.astype(np.uint8)\n",
    "        frame=frame.transpose([1,0,2])\n",
    "        return frame\n",
    "\n",
    "\n",
    "    def step(self,action,mode='Player'):\n",
    "        self.reward = 0\n",
    "        self.snake.move(mode,action)\n",
    "        if self.snake.body[0].pos == self.snack.pos:\n",
    "            self.reward=2\n",
    "            self.score+=self.reward\n",
    "\n",
    "            self.steps+=self.reward_steps\n",
    "            self.snake.addCube()\n",
    "            self.snack = Cube(randomSnack(self.rows, self.snake), color=(0, 255, 0))\n",
    "            self.render()\n",
    "            self.next_state=self.get_curr_frame()\n",
    "        else:\n",
    "            self.render()\n",
    "            self.next_state=self.get_curr_frame()\n",
    "            for x in range(len(self.snake.body)):\n",
    "                if self.snake.body[x].pos in list(map(lambda z: z.pos, self.snake.body[x + 1:])) or self.snake.head.pos[0] < 0 or self.snake.head.pos[0] > 19 or \\\n",
    "                        self.snake.head.pos[1] < 0 or self.snake.head.pos[1] > 19 or not self.steps:\n",
    "                    self.reward=-1\n",
    "                    self.score+=self.reward\n",
    "                    #print('THIS IS THE END!!!')\n",
    "                    #print(self.reward)\n",
    "                    #print(self.score)\n",
    "                    #print('Score: ', len(s.body)-1)\n",
    "                    #message_box('You Lost', 'Play again')\n",
    "                    self.next_state=None\n",
    "                    print('end')\n",
    "                    #return (self.reward,self.next_state)\n",
    "                    #self.reset()\n",
    "                    break\n",
    "            \n",
    "        self.steps-=1\n",
    "\n",
    "        return (self.reward,self.next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=deque(maxlen=5)\n",
    "for i in range(5):\n",
    "    q.append(i)\n",
    "q\n",
    "random.sample(q,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self,capacity):\n",
    "        self.memory=deque(maxlen=capacity)\n",
    "\n",
    "    def push(self,*args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self,batch_size):\n",
    "        return random.sample(self.memory,batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self,outputs):\n",
    "        super(DQN,self).__init__()\n",
    "        \n",
    "        #self.conv1=nn.Conv2d(3,32,3,1,1)\n",
    "        #self.bn1=nn.BatchNorm2d(32)\n",
    "        \n",
    "        #self.conv2=nn.Conv2d(32,64,3,1,1)\n",
    "        #self.bn2=nn.BatchNorm2d(64)\n",
    "        \n",
    "        #self.conv3=nn.Conv2d(64,64,3,1,1)\n",
    "        #self.bn3=nn.BatchNorm2d(64)\n",
    "\n",
    "        #self.pool=nn.MaxPool2d(2)\n",
    "        #self.pool1=nn.MaxPool2d(5)\n",
    "\n",
    "        self.fc1=nn.Linear(20*20*3+4,512)\n",
    "        self.fc2=nn.Linear(512,128)\n",
    "        self.fc3=nn.Linear(128,64)\n",
    "        self.fc4=nn.Linear(64,outputs)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #print(x.shape)\n",
    "        #x=F.relu(self.conv1(x))\n",
    "        #print(x.shape)\n",
    "        #x=F.relu(self.conv2(x))\n",
    "        #print(x.shape)\n",
    "        #x = self.pool(F.relu(self.conv3(x)))\n",
    "        #print(x.shape)\n",
    "        #print(x.shape)\n",
    "        #x=x.view(-1,20*20*3)\n",
    "\n",
    "        #x=torch.cat((x,features),1)\n",
    "        #print(x.shape)\n",
    "        #print(type(x))\n",
    "        x=torch.sigmoid(self.fc1(x))\n",
    "        x=torch.sigmoid(self.fc2(x))\n",
    "        x=torch.sigmoid(self.fc3(x))\n",
    "        x=self.fc4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawGrid(w,rows,surface):\n",
    "    sizeBtwn=w//rows\n",
    "    x=0\n",
    "    y=0\n",
    "    for l in range(rows):\n",
    "        x+=sizeBtwn\n",
    "        y+=sizeBtwn\n",
    "\n",
    "        pygame.draw.line(surface,(255,255,255),(x,0),(x,w))\n",
    "        pygame.draw.line(surface, (255, 255, 255), (0, y), (w, y))\n",
    "\n",
    "def redrawWindow(surface,rows,width,s,snack):\n",
    "    #global rows,width,s,snack\n",
    "    surface.fill((0, 0, 0))\n",
    "    s.draw(surface)\n",
    "    snack.draw(surface)\n",
    "    #drawGrid(width,rows,surface)\n",
    "    pygame.display.update()\n",
    "\n",
    "def randomSnack(rows,item):\n",
    "\n",
    "    positions=item.body\n",
    "    while True:\n",
    "        x=random.randrange(rows)\n",
    "        y=random.randrange(rows)\n",
    "        if len(list(filter(lambda z:z.pos==(x,y),positions)))>0:\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    return (x,y)\n",
    "\n",
    "def message_box(subject,content):\n",
    "    root=tk.Tk()\n",
    "    root.attributes(\"-topmost\",True)\n",
    "    root.withdraw()\n",
    "    messagebox.showinfo(subject,content)\n",
    "    try:\n",
    "        root.destroy()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def get_screen(frame,feature):\n",
    "    # the get frame of env returns H*W*C\n",
    "    #get the screen of snake and change it to pytorch image format B*C*H*W\n",
    "    #env.render()\n",
    "    feature=torch.tensor(feature)/20.0\n",
    "    if frame is None:\n",
    "        return None\n",
    "    screen=frame.transpose([2,0,1])\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # add .to(device) when train on GPU\n",
    "    #return resize(screen).unsqueeze(0).to(device)\n",
    "    screen=resize(screen)\n",
    "    screen=screen.view(-1,20*20*3)\n",
    "    #print(screen.shape)\n",
    "    screen=torch.cat((screen,feature),1)\n",
    "    #print(screen.shape)\n",
    "    #screen=screen.unsqueeze(0)\n",
    "    #print(screen.shape)\n",
    "    return screen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.arange(100000)\n",
    "b=a.to(device)\n",
    "del b\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize=T.Compose([T.ToPILImage(),\n",
    "                  T.Resize(20, interpolation=Image.CUBIC),\n",
    "                  T.ToTensor()])\n",
    "\n",
    "Transition=namedtuple('Transition',\n",
    "                        ('state', 'action', 'reward', 'next_state'))\n",
    "\n",
    "\n",
    "BATCH_SIZE=16\n",
    "GAMMA=0.99\n",
    "EPS_START=0.9\n",
    "EPS_END=0.1\n",
    "EPS_DECAY=1000000\n",
    "TARGET_UPDATE=10\n",
    "NUM_ACTIONS=4\n",
    "SCREEN_WIDTH=200\n",
    "SCREEN_HEIGHT=200\n",
    "steps_done=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del policy_net\n",
    "del target_net\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#memory=ReplayMemory(50000)\n",
    "\n",
    "\n",
    "#remember .to(device) for GPU\n",
    "#feature=torch.tensor([[1,2]],dtype=torch.float32)\n",
    "policy_net=DQN(NUM_ACTIONS).to(device)\n",
    "target_net=DQN(NUM_ACTIONS).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "#target_net.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optim.Adam(policy_net.parameters(),lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.init()\n",
    "env=Environment(SCREEN_WIDTH,SCREEN_HEIGHT,steps=50)\n",
    "env.reset()\n",
    "env.render()\n",
    "s=env.get_curr_frame()\n",
    "\n",
    "feature=torch.tensor([[1,2]],dtype=torch.float32)\n",
    "s=get_screen(s,feature)\n",
    "s_gpu=s.to(device)\n",
    "s_gpu=torch.cat((s_gpu,s_gpu))\n",
    "policy_net(s_gpu).max(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.init()\n",
    "env=Environment(SCREEN_WIDTH,SCREEN_HEIGHT,steps=50)\n",
    "env.reset()\n",
    "env.render()\n",
    "s=env.get_curr_frame()\n",
    "#print(resize(s).numpy().transpose((1,2,0)))\n",
    "plt.figure(1)\n",
    "plt.imshow(resize(s).numpy().transpose((1,2,0)))\n",
    "print(env.snake.head.pos)\n",
    "a=1\n",
    "r,ns=env.step(a,'Agent')\n",
    "plt.figure(2)\n",
    "plt.imshow(ns)\n",
    "print(env.snake.head.pos)\n",
    "#s_torch=get_screen(s)\n",
    "#s_torch=torch.cat([s_torch,s_torch,s_torch,s_torch],dim=1)\n",
    "#out=policy_net(s_torch)\n",
    "#s_torch.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.render()\n",
    "s=env.get_curr_frame()\n",
    "\n",
    "plt.figure(1)\n",
    "plt.imshow(s)\n",
    "print(env.snake.head.pos)\n",
    "a=1\n",
    "r,ns=env.step(a,'Agent')\n",
    "plt.figure(2)\n",
    "plt.imshow(ns)\n",
    "print(env.snake.head.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=input('enter the key: ')\n",
    "print(typ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## behave like the Agent 0:up 1:down 2:left 3:right\n",
    "pygame.init()\n",
    "env=Environment(SCREEN_WIDTH,SCREEN_HEIGHT,steps=50)\n",
    "env.reset()\n",
    "env.render()\n",
    "s=env.get_curr_frame()\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.imshow(s)\n",
    "#print('Snake pos: {}, Apple pos: {}'.format(env.snake.head.pos,env.snack.pos))\n",
    "\n",
    "\n",
    "while env.snake.head.pos!=env.snack.pos:\n",
    "\n",
    "    plt.figure(1)\n",
    "    #plt.clf()\n",
    "    plt.imshow(s)\n",
    "    prev_snake_head_pos=env.snake.head.pos\n",
    "    snack_pos=env.snack.pos\n",
    "    \n",
    "    plt.pause(0.001)\n",
    "    a=input('enter the key: ')\n",
    "    a=int(a)\n",
    "    _,s=env.step(a,'Agent')\n",
    "    print('Prev_pos: {}, Snake pos: {}, Apple pos: {}'.format(type(prev_snake_head_pos),env.snake.head.pos,snack_pos))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### a=np.array((1,2))-np.array((3,4))\n",
    "np.sqrt(np.sum(a**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_final_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack.append(s_torch)\n",
    "stack.append(s_torch)\n",
    "stack.append(s_torch)\n",
    "stack.append(s_torch)\n",
    "s_torch=torch.cat(list(stack),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_torch.shape\n",
    "out=policy_net(s_torch)\n",
    "out.max(1)[1].view(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.randint(4,(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = deque(maxlen=100)\n",
    "scores=deque(maxlen=100)\n",
    "\n",
    "def plot_all(fig_num=2):\n",
    "    plt.figure(fig_num)\n",
    "    plt.subplot(121)\n",
    "    plt.clf()\n",
    "\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(episode_durations)\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(episode_durations) >= 100:\n",
    "        means = np.mean(episode_durations)\n",
    "\n",
    "        plt.plot(means)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.clf()\n",
    "\n",
    "    plt.title('Score')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Score')\n",
    "    plt.plot(scores)\n",
    "    if len(scores)>=100:\n",
    "        mmeans=np.mean(scores)\n",
    "\n",
    "        plt.plot(mmeans)\n",
    "    plt.pause(0.001)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    gc.collect()\n",
    "    if len(memory.memory)<BATCH_SIZE:\n",
    "        return\n",
    "#     if len(reward_memory.memory)<BATCH_SIZE:\n",
    "#         return\n",
    "#     if len(eat_memory.memory)<BATCH_SIZE:\n",
    "#         return\n",
    "    \n",
    "    #a=int(BATCH_SIZE*0.5)\n",
    "    #b=int(BATCH_SIZE*0.5)\n",
    "#     c=int(BATCH_SIZE*0.2)\n",
    "    \n",
    "#     transitions=memory.sample(a)+reward_memory.sample(b)+eat_memory.sample(c)\n",
    "    transitions=memory.sample(BATCH_SIZE)\n",
    "    #print(type(transitions))\n",
    "    random.shuffle(transitions)\n",
    "    batch=Transition(*zip(*transitions))\n",
    "    \n",
    "    #device=device for GPU\n",
    "    non_final_mask=torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)),device=device,dtype=torch.bool)\n",
    "    #print('non final states mask: ',non_final_mask.shape)\n",
    "    non_final_next_states=torch.cat([s for s in batch.next_state if s is not None]).to(device)\n",
    "    #print('non final states: ',non_final_next_states.shape)\n",
    "    state_batch=torch.cat(batch.state).to(device)\n",
    "    #print('state_batch: ',state_batch.shape)\n",
    "    action_batch=torch.cat(batch.action).to(device)\n",
    "    #print('action_batch: ',action_batch.shape)\n",
    "    reward_batch=torch.cat(batch.reward).to(device)\n",
    "    #print('reward_batch: ',reward_batch)\n",
    "    state_action_values=policy_net(state_batch).gather(1,action_batch)\n",
    "    #print('state_action_values: ',state_action_values)\n",
    "    next_state_values=torch.zeros(BATCH_SIZE).to(device)\n",
    "    #print('next_state_values: ',next_state_values)\n",
    "#     non_final_mask\n",
    "#     non_final_next_states\n",
    "#     state_batch\n",
    "#     action_batch\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask]=target_net(non_final_next_states).max(1)[0]\n",
    "        #print('next_state_values: ',next_state_values)\n",
    "    expected_state_action_values=(next_state_values*GAMMA)+reward_batch\n",
    "    #print('expected_state_action_values: ',expected_state_action_values.unsqueeze(1))\n",
    "    loss=F.l1_loss(state_action_values,expected_state_action_values.unsqueeze(1))\n",
    "    #print('losss: ',loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1,1)\n",
    "    optimizer.step()\n",
    "    del non_final_mask\n",
    "    del non_final_next_states\n",
    "    del state_batch\n",
    "    del action_batch\n",
    "    del reward_batch\n",
    "    del state_action_values\n",
    "    del next_state_values\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.init()\n",
    "env=Environment(SCREEN_WIDTH,SCREEN_HEIGHT,steps=50)\n",
    "env.reset()\n",
    "env.render()\n",
    "s=env.get_curr_frame()\n",
    "\n",
    "plt.imshow(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions=memory.sample(int(BATCH_SIZE*0.75))+reward_memory.sample(int(BATCH_SIZE*0.25))\n",
    "print(type(transitions))\n",
    "random.shuffle(transitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack.append('a')\n",
    "stack.append('b')\n",
    "stack.append('c')\n",
    "stack.append('d')\n",
    "stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack.append('e')\n",
    "stack\n",
    "torch.cat([s_torch,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=ReplayMemory(3)\n",
    "m.push('a','b','c','d')\n",
    "m.memory[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.init()\n",
    "env=Environment(SCREEN_WIDTH,SCREEN_HEIGHT,steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_episodes=500\n",
    "\n",
    "num_episodes=20000\n",
    "\n",
    "env.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tensor([1])\n",
    "a.item()==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(memory.sample(2)+memory.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations=[]\n",
    "scores=[]\n",
    "# frame_stack=4\n",
    "memory=ReplayMemory(10000)\n",
    "#reward_memory=ReplayMemory(10000)\n",
    "#eat_memory=ReplayMemory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# stack=deque(maxlen=frame_stack)\n",
    "for i_init in range(init_episodes):\n",
    "    env.reset()\n",
    "    s=0\n",
    "#     for i in range(frame_stack):\n",
    "        \n",
    "#         env.render()\n",
    "#         state=env.get_curr_frame()\n",
    "#         state=get_screen(state)\n",
    "#         stack.append(state)\n",
    "    #env.snack.pos\n",
    "    for t in count():\n",
    "        env.render()\n",
    "        frame=env.get_curr_frame()\n",
    "        state=get_screen(frame)\n",
    "#         plt.figure(3)\n",
    "#         plt.clf()\n",
    "#         plt.imshow(frame)\n",
    "#         plt.pause(0.001)\n",
    "        #state=torch.cat(list(stack),dim=1)\n",
    "        state_gpu=state.to(device)\n",
    "        action=torch.randint(4,(1,1))\n",
    "        \n",
    "        prev_snake_pos=env.snake.head.pos\n",
    "        snack_pos=env.snack.pos\n",
    "\n",
    "        #print(action)\n",
    "        reward,next_state=env.step(action.item(),'Agent')\n",
    "        \n",
    "            \n",
    "        \n",
    "        curr_snake_pos=env.snake.head.pos\n",
    "        \n",
    "        #small change should be positive if snake getting close to snack\n",
    "        #else negative\n",
    "        #prev_d=np.array(prev_snake_pos)-np.array(snack_pos)\n",
    "        #prev_d=np.sqrt(np.sum(prev_d**2))\n",
    "        prev_d=np.array(prev_snake_pos)-np.array(snack_pos)\n",
    "        prev_d=np.sum(np.abs(prev_d))\n",
    "        curr_d=np.array(curr_snake_pos)-np.array(snack_pos)\n",
    "        curr_d=np.sum(np.abs(curr_d))\n",
    "        \n",
    "        small_change=(prev_d-curr_d)\n",
    "        \n",
    "        \n",
    "        small_change=small_change*0.01 if small_change<0 else small_change*0.01\n",
    "        \n",
    "        small_change-=curr_d*0.001\n",
    "        #print(small_change)\n",
    "        #print(small_change)\n",
    "        reward+=small_change\n",
    "        #print(reward)\n",
    "        #break\n",
    "        #print(reward)\n",
    "        s+=reward\n",
    "        #plot_env()       \n",
    "        reward=torch.tensor([reward],dtype=torch.float32)\n",
    "        next_state=get_screen(next_state)\n",
    "#         if next_state is not None:\n",
    "#             stack.append(next_state)\n",
    "#             next_state=torch.cat(list(stack),dim=1)\n",
    "        #print(reward)\n",
    "    \n",
    "        if reward.item()>0:\n",
    "            reward_memory.push(state,action,reward,next_state)\n",
    "            \n",
    "        memory.push(state,action,reward,next_state)\n",
    "\n",
    "        optimize_model()\n",
    "\n",
    "        del state_gpu\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if next_state is None:\n",
    "            episode_durations.append(t + 1)\n",
    "\n",
    "            scores.append(s)\n",
    "\n",
    "            plot_all(fig_num=2)\n",
    "\n",
    "            break\n",
    "    print(len(memory.memory))\n",
    "    print(len(reward_memory.memory))\n",
    "    #print(len(eat_memory.memory))\n",
    "    \n",
    "    if i_init%TARGET_UPDATE==0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "print('initialization finished')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reward_memory.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1,2)+(3,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([(1,2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#episode_durations=[]\n",
    "#scores=[]\n",
    "#frame_stack=4\n",
    "#memory=ReplayMemory(10000)\n",
    "#stack=deque(maxlen=frame_stack)\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    s=0\n",
    "#     for i in range(frame_stack):\n",
    "#         env.render()\n",
    "#         state=env.get_curr_frame()\n",
    "#         state=get_screen(state)\n",
    "#         stack.append(state)\n",
    "    \n",
    "    for t in count():\n",
    "        \n",
    "        frame=env.get_curr_frame()\n",
    "        snack_pos=env.snack.pos\n",
    "        \n",
    "        prev_snake_pos=env.snake.head.pos\n",
    "        \n",
    "        state=get_screen(frame,[snack_pos+prev_snake_pos])\n",
    "#         plt.figure(3)\n",
    "#         plt.clf()\n",
    "#         plt.imshow(frame)\n",
    "#         plt.pause(0.001)\n",
    "        #state=torch.cat(list(stack),dim=1)\n",
    "        state_gpu=state.to(device)\n",
    "        action=select_action(state_gpu)\n",
    "        action=action.cpu()\n",
    "        #print(action)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        reward,next_state=env.step(action.item(),'Agent')\n",
    "        \n",
    "        \n",
    "        curr_snake_pos=env.snake.head.pos\n",
    "        \n",
    "        #small change should be positive if snake getting close to snack\n",
    "        #else negative\n",
    "        \n",
    "        prev_d=np.array(prev_snake_pos)-np.array(snack_pos)\n",
    "        prev_d=np.sqrt(np.sum(prev_d**2))\n",
    "        curr_d=np.array(curr_snake_pos)-np.array(snack_pos)\n",
    "        curr_d=np.sqrt(np.sum(curr_d**2))\n",
    "        #small_change=(prev_d-curr_d)\n",
    "        #small_change=small_change*0.01 \n",
    "        if prev_d>=curr_d:\n",
    "            reward+=0.01*np.abs(0.99*curr_d-prev_d)\n",
    "        else:\n",
    "            reward-=0.02*np.abs(0.99*curr_d-prev_d)\n",
    "\n",
    "        \n",
    "        #small_change-=curr_d*0.001\n",
    "        #print(small_change)\n",
    "        \n",
    "        \n",
    "        s+=reward\n",
    "        #plot_env()       \n",
    "        reward=torch.tensor([reward],dtype=torch.float32)\n",
    "        \n",
    "\n",
    "        \n",
    "        next_state=get_screen(next_state,[snack_pos+ curr_snake_pos])\n",
    "#         if reward.item()>0:\n",
    "#             reward_memory.push(state,action,reward,next_state)\n",
    "#         if next_state is not None:\n",
    "#             stack.append(next_state)\n",
    "#             next_state=torch.cat(list(stack),dim=1)\n",
    "        #print(next_state.shape)\n",
    "        memory.push(state,action,reward,next_state)\n",
    "        \n",
    "        optimize_model()\n",
    "        \n",
    "        del state_gpu\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if next_state is None:\n",
    "            episode_durations.append(t + 1)\n",
    "\n",
    "            scores.append(s)\n",
    "            \n",
    "            plot_all(fig_num=3)\n",
    "\n",
    "            break\n",
    "    if i_episode%TARGET_UPDATE==0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    if i_episode%100==0:\n",
    "        torch.save(policy_net.state_dict(),'snake')\n",
    "            \n",
    "print('Complete')\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(),'snake')\n",
    "import pickle \n",
    "with open('eps_duration.txt','wb') as fp:\n",
    "    pickle.dump(episode_durations,fp)\n",
    "with open('score.txt','wb') as fp:\n",
    "    pickle.dump(scores,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trans in reward_memory.memory:\n",
    "    if trans.next_state is np.ndarray:\n",
    "        reawrd_memory.remove(trans)\n",
    "for trans in memory.memory:\n",
    "    if trans.next_state is np.ndarray:\n",
    "        memory.remove(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_episode in range(num_episodes):\n",
    "    env.reset()\n",
    "    s=0\n",
    "#     for i in range(frame_stack):\n",
    "#         env.render()\n",
    "#         state=env.get_curr_frame()\n",
    "#         state=get_screen(state)\n",
    "#         stack.append(state)\n",
    "    for t in count():\n",
    "        frame=env.get_curr_frame()\n",
    "        snack_pos=env.snack.pos\n",
    "        \n",
    "        prev_snake_pos=env.snake.head.pos\n",
    "        \n",
    "        state=get_screen(frame,[snack_pos+prev_snake_pos])\n",
    "        plt.figure(3)\n",
    "        plt.clf()\n",
    "        plt.imshow(frame)\n",
    "        plt.pause(0.001)\n",
    "\n",
    "        state_gpu=state.to(device)\n",
    "        action=select_action(state_gpu)\n",
    "        action=action.cpu()\n",
    "        #print(action)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        reward,next_state=env.step(action.item(),'Agent')\n",
    "        \n",
    "        \n",
    "        curr_snake_pos=env.snake.head.pos\n",
    "\n",
    "        #small change should be positive if snake getting close to snack\n",
    "        #else negative\n",
    "\n",
    "        prev_d=np.array(prev_snake_pos)-np.array(snack_pos)\n",
    "        prev_d=np.sqrt(np.sum(prev_d**2))\n",
    "        curr_d=np.array(curr_snake_pos)-np.array(snack_pos)\n",
    "        curr_d=np.sqrt(np.sum(curr_d**2))\n",
    "        #small_change=(prev_d-curr_d)\n",
    "        #small_change=small_change*0.01 \n",
    "        if prev_d>=curr_d:\n",
    "            reward+=0.01*np.abs(0.99*curr_d-prev_d)\n",
    "        else:\n",
    "            reward-=0.02*np.abs(0.99*curr_d-prev_d)\n",
    "\n",
    "        \n",
    "        #small_change-=curr_d*0.001\n",
    "        #print(small_change)\n",
    "        \n",
    "        \n",
    "        s+=reward\n",
    "        #plot_env()       \n",
    "        reward=torch.tensor([reward],dtype=torch.float32)\n",
    "        \n",
    "\n",
    "        \n",
    "        next_state=get_screen(next_state,[snack_pos+ curr_snake_pos])\n",
    "#         if reward.item()>0:\n",
    "#             reward_memory.push(state,action,reward,next_state)\n",
    "#         if next_state is not None:\n",
    "#             stack.append(next_state)\n",
    "#             next_state=torch.cat(list(stack),dim=1)\n",
    "        #print(next_state.shape)\n",
    "        if next_state is not None:\n",
    "            pass\n",
    "#             stack.append(next_state)\n",
    "#             next_state=torch.cat(list(stack),dim=1)\n",
    "        else:\n",
    "            print(env.steps)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[s for s in batch.next_state if s is not None]\n",
    "for t in x:\n",
    "    print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_final_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
